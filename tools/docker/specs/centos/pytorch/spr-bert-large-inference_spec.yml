releases:
  versioned:
    tag_specs:
    - '{pytorch-multistage}{spr-bert-large-inference}'
slice_sets:
  spr-bert-large-inference:
    - add_to_name: -spr-bert-large-inference
      dockerfile_subdirectory: pytorch
      args:
        - PYTORCH_IMAGE=model-zoo
        - PYTORCH_TAG=pytorch-ipex-spr
        - PACKAGE_NAME=pytorch-spr-bert-large-inference
      partials:
        - pytorch/gcc-and-utils
        - pytorch/tcmalloc
        - model_package
        - pytorch/models/bert-inference-dependencies
        - pytorch/spr-release-conda-env
        - pytorch/spr-dnnl-max-var
        - pytorch/tcmalloc-entrypoint
      files:
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/enable_ipex_for_squad.diff
          destination: quickstart/enable_ipex_for_squad.diff
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json
          destination: quickstart/configure.json
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/run_accuracy.sh
          destination: quickstart/run_accuracy.sh
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/run_calibration.sh
          destination: quickstart/run_calibration.sh
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/run_multi_instance_realtime.sh
          destination: quickstart/run_multi_instance_realtime.sh
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/run_multi_instance_throughput.sh
          destination: quickstart/run_multi_instance_throughput.sh
      wrapper_package_files:
        - source: output/pytorch-spr-bert-large-inference.tar.gz
          destination: model_packages/pytorch-spr-bert-large-inference.tar.gz
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/build.sh
          destination: build.sh
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/run.sh
          destination: run.sh
        - source: dockerfiles/pytorch/pytorch-spr-bert-large-inference.Dockerfile
          destination: pytorch-spr-bert-large-inference.Dockerfile
        - source: LICENSE
          destination: licenses/LICENSE
        - source: third_party
          destination: licenses/third_party
        - source: quickstart/language_modeling/pytorch/bert_large/inference/cpu/README_SPR.md
          destination: README.md
      documentation:
        - docs:
          - name: Title
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/title.md
          - name: Description
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/description.md
          - name: Model Package
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/wrapper_package.md
          - name: Quickstart
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/quickstart.md
          - name: Datasets
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/dataset.md
          - name: Container build
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/container_build.md
          - name: Docker
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/docker_spr.md
          - name: License
            uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu/.docs/license.md
          name: README_SPR.md
          text_replace:
            <model name>: BERT Large
            <mode>: inference
            <package name>: pytorch-spr-bert-large-inference.tar.gz
            <package dir>: pytorch-spr-bert-large-inference
            <docker image>: model-zoo:pytorch-bert-large-inference
          uri: models/quickstart/language_modeling/pytorch/bert_large/inference/cpu
